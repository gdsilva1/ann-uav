\section{Data Generation}

Since the script of \textcite{geronel2023} provides the control torque \(\mathbf{\tau}\) as input and the state-space \(\mathbf{x}_s\) as output vector through dynamic and control equations, the NN goal developed is to go in the opposite direction, as a inverse function: take \(\mathbf{x}_s\) as the input vector and predict the \(\mathbf{\tau}_{\eta}\) vector as output.
Modifications in the script are minimal.
The time is a discrete vector with \SI{200}{s} and step 0.01, therefore the time vector has \(1\times 20001\) dimension.
The ``extra'' value of time is the zero value.

The output vector \(\mathbf{T}\) has \(20001\times 4\) dimension the and the input vector \(\mathbf{X}_s\) has  \(20001\times 12\) dimension:
%
\begin{align}
    \mathbf{T} &= \begin{bmatrix}
        U_1 & U_2 & U_3 & U_4 \\
        \vdots       & \vdots       & \vdots       & \vdots  \\
    \end{bmatrix} 
    \label{eq:tau_input} \\
    \setcounter{MaxMatrixCols}{13}
    \mathbf{X}_s &=
    \begin{bmatrix}
        x&y&z&\phi&\theta&\psi&\dot{x}&\dot{y}&\dot{z}&\dot{\phi}&\dot{\theta}&\dot{\psi} \\
        \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots & \vdots 
    \end{bmatrix}
    \label{eq:xs_output}
\end{align}

Circular trajectory was arbitrary selected as starting point.
By a loop, it was generated 1000 different trajectories changing the position  by adding random noise.
Input and output vector were stored in a MATLAB variable and exported through the \texttt{.mat} extension to be used inside the Python environment.

\section{Algorithm Overview}

The first approach for the NN is to use the raw data, both for input and output and do the training.
Even though it works, it does not give the proper result.
Therefore, the preprocessing of the data is mandatory to get the best results.
This way, all input and output data were normalized in order to get them all standardized.

The normalization is in L2 form, from the \emph{sklearn.preprocessing.normalize} function, applied in each matrix column.
From the trained NN, all input data should be normalized and naturally the output also will be normalized.
However, the control forces (output data) can not be normalized to be useful, but there is no ``denormalized'' correspondent matrix to the output data from the NN.

To solve this problem, a second NN was created to be able to denormalize the output data.
When preprocessing the data, as the normalization is done, both norms of the input and output data are stored and the second NN is made from them.
The~\cref{fig:nns_scheme} show how the data and the NNS are related for the training. 
When the training and the validation is done, the ready-to-use model will perform as shown in the~\cref{fig:full_scheme} scheme.

\begin{figure}[!htb]
    \centering
    \caption[Data and neural networks relation]{Data and neural networks relation. The training data in the extremes are the ones generated by the white box parametric model.}
    \includesvg[pretex=\footnotesize]{figures/3methodology/nns_scheme.svg}

    {\footnotesize Source: prepared by the author.}
    \label{fig:nns_scheme}
\end{figure}

\begin{figure}[!htb]
    \centering
    \caption[Model in production]{Model in production. The scheme shows how the process returns the control forces from the state space.}
    \includesvg[pretex=\footnotesize]{figures/3methodology/full_scheme.svg}

    {\footnotesize Source: prepared by the author.}
    \label{fig:full_scheme}
\end{figure}

\section{Neural Network Modeling}

The NN 1 is responsible for, from the normalized state space, to return the normalized control forces.
The problem is considered as a regression problem, as the~\cref{eq:function_training_model_uav} shows. Input and output data are all matrices.
%
\begin{subequations}\label{eq:function_training_model_uav}
    \begin{align}
        &f\big(x,y,\ldots,\dot{\theta},\dot{\psi}\big) = \langle U_1, U_2, U_3, U_4 \rangle \\
        &f(\mathbf{X}_s) = \mathbf{T}
    \end{align}
\end{subequations}

% \begin{figure}[!htb]
%     \centering
%     \caption[Schematic model of the neural network for the normalized data]{Schematic model of the neural network for the normalized data. The input layer receives every element of the \(\mathbf{X}_s\) and returns every element of \(\mathbf{T}\).}
%     \includesvg[pretex=\footnotesize]{./figures/3review/nn/nn2.svg}

%     {\footnotesize Source: prepared by the author.}
%     \label{fig:nn1_scheme}
% \end{figure}

The  NN 2 is responsible for, from the raw data, to get the norms from each input and output data of the NN 1.
The problem is considered as a regression problem.
Input and output data are all vectors.

% \begin{figure}[!htb]
%     \centering
%     \caption[Schematic model of the neural network for the norms]{Schematic model of the neural network for the norms. The input data receives the state space norm and return the correspondent force control norm. There are two hidden layers, each one with 64 neurons.}
%     \includesvg[pretex=\footnotesize]{./figures/3review/nn/nn4.svg}

%     {\footnotesize Source: prepared by the author.}
%     \label{fig:nn2_scheme}
% \end{figure}

Both NNs have similar parameters, being the main difference the input and output layer size.
Characteristics of them are provided in the~\cref{tab:nns_char}.

\begin{table}[!htb]
    \centering
    \caption{Parameters of the neural networks and their training}
    \begin{tblr}{
         row{even} = {black!5},
         colspec={lcc}
    }
    \toprule
    Parameters & NN 1 & NN 2 \\
    \midrule
    Input layer neurons & 12 & 1  \\
    Output layer neurons & 4 & 1  \\
    Hidden layers neurons & 128 & 128  \\
    Hidden layers & 8 & 8  \\
    Activation function & ReLU & ReLU  \\
    Loss function & MSE & MSE \\
    Batch size & 1 & 1 \\
    Train/test split & 0.8/0.2 & 0.8/0.2 \\
    \bottomrule
    \end{tblr}
    \label{tab:nns_char}
\end{table}

